{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 357
    },
    "id": "KwohNxADZ3cQ",
    "outputId": "c9bf2fc0-3018-4e6b-961c-545e3b52cf04"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Oct 21 11:57:48 2020       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 455.23.05    Driver Version: 418.67       CUDA Version: 10.1     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
      "| N/A   54C    P0    30W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
      "|                               |                      |                 ERR! |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "xP16GtfYaYU1"
   },
   "outputs": [],
   "source": [
    "!mv kaggle.json ~/.kaggle/\n",
    "!chmod 600 ~/.kaggle/kaggle.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "id": "fY7zt9hKZoky",
    "outputId": "2df3f0fc-b6ae-4da9-8614-e2b47d3e8e4f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading celeba-dataset.zip to /content\n",
      "100% 1.33G/1.33G [00:26<00:00, 76.2MB/s]\n",
      "100% 1.33G/1.33G [00:26<00:00, 54.4MB/s]\n"
     ]
    }
   ],
   "source": [
    "!kaggle datasets download -d jessicali9530/celeba-dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "ZJWR1k5bacy_"
   },
   "outputs": [],
   "source": [
    "!unzip -q celeba-dataset.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "TOts1O2bax8o"
   },
   "outputs": [],
   "source": [
    "!rm celeba-dataset.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "xcxnGSS1buTb"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import PIL.Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "hbvgVSv8doQr"
   },
   "outputs": [],
   "source": [
    "IMG_HEIGHT = 128\n",
    "IMG_WIDTH = 128\n",
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "qeVM0l2ddKsZ"
   },
   "outputs": [],
   "source": [
    "def load(file_path):\n",
    "    img = tf.io.read_file(file_path)\n",
    "    img = tf.io.decode_image(img, channels=3, expand_animations = False)\n",
    "    img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "    img = tf.image.resize(img, [IMG_HEIGHT, IMG_WIDTH])\n",
    "    return img*2 - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "y_NJYL2V0Qid"
   },
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.experimental.AUTOTUNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "nFf6MN3vrLzf"
   },
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.list_files(\"/content/img_align_celeba/img_align_celeba/*\")\n",
    "dataset = dataset.shuffle(len(dataset))\n",
    "dataset = dataset.map(load, num_parallel_calls=AUTOTUNE)\n",
    "dataset = dataset.batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "4jvD_eMxtFG3"
   },
   "outputs": [],
   "source": [
    "def get_generator():\n",
    "    inp = layers.Input(shape = (100,))\n",
    "    x = layers.Dense(8*8*512, use_bias=False)(inp)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.LeakyReLU()(x)\n",
    "    x = layers.Reshape((8,8,512))(x)\n",
    "\n",
    "    x = layers.Conv2DTranspose(512, kernel_size=(4,4), strides=(1,1), padding='same', use_bias=False)(x)\n",
    "    x = layers.BatchNormalization()(x)          # (8, 8, 512)\n",
    "    x = layers.LeakyReLU()(x)\n",
    "\n",
    "    x = layers.Conv2DTranspose(256, kernel_size=(4,4), strides=(2,2), padding='same', use_bias=False)(x)\n",
    "    x = layers.BatchNormalization()(x)          # (16, 16, 256)\n",
    "    x = layers.LeakyReLU()(x)\n",
    "\n",
    "    x = layers.Conv2DTranspose(128, kernel_size=(4,4), strides=(2,2), padding='same', use_bias=False)(x)\n",
    "    x = layers.BatchNormalization()(x)          # (32, 32, 128)\n",
    "    x = layers.LeakyReLU()(x)\n",
    "\n",
    "    x = layers.Conv2DTranspose(64, kernel_size=(4,4), strides=(2,2), padding='same', use_bias=False)(x)\n",
    "    x = layers.BatchNormalization()(x)          # (64, 64, 64)\n",
    "    x = layers.LeakyReLU()(x)\n",
    "\n",
    "    x = layers.Conv2DTranspose(32, kernel_size=(4,4), strides=(2,2), padding='same', use_bias=False)(x)\n",
    "    x = layers.BatchNormalization()(x)          # (128, 128, 32)\n",
    "    x = layers.LeakyReLU()(x)\n",
    "\n",
    "    x = layers.Conv2D(3, kernel_size=(4,4), strides=(1,1), padding='same', activation='tanh')(x)\n",
    "\n",
    "    model = tf.keras.Model(inputs=inp, outputs=x)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 867
    },
    "id": "9SvHn-bdzxFd",
    "outputId": "40cda4c0-be02-4ca9-b286-3d1f4fd3b403"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 100)]             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32768)             3276800   \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 32768)             131072    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)    (None, 32768)             0         \n",
      "_________________________________________________________________\n",
      "reshape_1 (Reshape)          (None, 8, 8, 512)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_5 (Conv2DTr (None, 8, 8, 512)         4194304   \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 8, 8, 512)         2048      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_7 (LeakyReLU)    (None, 8, 8, 512)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_6 (Conv2DTr (None, 16, 16, 256)       2097152   \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 16, 16, 256)       1024      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_8 (LeakyReLU)    (None, 16, 16, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_7 (Conv2DTr (None, 32, 32, 128)       524288    \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 32, 32, 128)       512       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_9 (LeakyReLU)    (None, 32, 32, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_8 (Conv2DTr (None, 64, 64, 64)        131072    \n",
      "_________________________________________________________________\n",
      "batch_normalization_10 (Batc (None, 64, 64, 64)        256       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_10 (LeakyReLU)   (None, 64, 64, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_9 (Conv2DTr (None, 128, 128, 32)      32768     \n",
      "_________________________________________________________________\n",
      "batch_normalization_11 (Batc (None, 128, 128, 32)      128       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_11 (LeakyReLU)   (None, 128, 128, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 128, 128, 3)       1539      \n",
      "=================================================================\n",
      "Total params: 10,392,963\n",
      "Trainable params: 10,325,443\n",
      "Non-trainable params: 67,520\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "g = get_generator()\n",
    "g.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "ZTPslonK0z28"
   },
   "outputs": [],
   "source": [
    "def get_discriminator():\n",
    "    inp = layers.Input(shape=(128,128,3))\n",
    "    x = layers.Conv2D(32, kernel_size=(4,4), strides=(1,1), padding='same')(inp)\n",
    "    x = layers.LeakyReLU()(x)                   # (128, 128, 32)\n",
    "\n",
    "    x = layers.Conv2D(64, kernel_size=(4,4), strides=(2,2), padding='same', use_bias=False)(x)\n",
    "    x = layers.BatchNormalization()(x)          # (64, 64, 64)\n",
    "    x = layers.LeakyReLU()(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "\n",
    "    x = layers.Conv2D(128, kernel_size=(4,4), strides=(2,2), padding='same', use_bias=False)(x)\n",
    "    x = layers.BatchNormalization()(x)          # (32, 32, 128)\n",
    "    x = layers.LeakyReLU()(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "\n",
    "    x = layers.Conv2D(256, kernel_size=(4,4), strides=(2,2), padding='same', use_bias=False)(x)\n",
    "    x = layers.BatchNormalization()(x)          # (16, 16, 256)\n",
    "    x = layers.LeakyReLU()(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "\n",
    "    x = layers.Conv2D(512, kernel_size=(4,4), strides=(2,2), padding='same', use_bias=False)(x)\n",
    "    x = layers.BatchNormalization()(x)          # (8, 8, 512)\n",
    "    x = layers.LeakyReLU()(x)\n",
    "\n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    x = layers.Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "    model = tf.keras.Model(inputs=inp, outputs=x)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 867
    },
    "id": "VqUIE-i24pJU",
    "outputId": "c145c039-b9a2-4784-abe4-7fef3cb132ea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         [(None, 128, 128, 3)]     0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 128, 128, 32)      1568      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_12 (LeakyReLU)   (None, 128, 128, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 64, 64, 64)        32768     \n",
      "_________________________________________________________________\n",
      "batch_normalization_12 (Batc (None, 64, 64, 64)        256       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_13 (LeakyReLU)   (None, 64, 64, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 64, 64, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 32, 32, 128)       131072    \n",
      "_________________________________________________________________\n",
      "batch_normalization_13 (Batc (None, 32, 32, 128)       512       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_14 (LeakyReLU)   (None, 32, 32, 128)       0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 32, 32, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 16, 16, 256)       524288    \n",
      "_________________________________________________________________\n",
      "batch_normalization_14 (Batc (None, 16, 16, 256)       1024      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_15 (LeakyReLU)   (None, 16, 16, 256)       0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 16, 16, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 8, 8, 512)         2097152   \n",
      "_________________________________________________________________\n",
      "batch_normalization_15 (Batc (None, 8, 8, 512)         2048      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_16 (LeakyReLU)   (None, 8, 8, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 32768)             0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 32768)             0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 32769     \n",
      "=================================================================\n",
      "Total params: 2,823,457\n",
      "Trainable params: 2,821,537\n",
      "Non-trainable params: 1,920\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "d = get_discriminator()\n",
    "d.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "prrDvREw4su2"
   },
   "outputs": [],
   "source": [
    "criterion = tf.keras.losses.BinaryCrossentropy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "aXIZ0t1t5z-U"
   },
   "outputs": [],
   "source": [
    "def discriminator_loss(real_output, fake_output):\n",
    "    # real_loss = criterion(tf.ones_like(real_output), real_output)\n",
    "    # fake_loss = criterion(tf.zeros_like(fake_output), fake_output)\n",
    "    # return real_loss + fake_loss\n",
    "    real_loss = criterion(tf.random.uniform(real_output.shape, minval=0.9, maxval=1), real_output)\n",
    "    fake_loss = criterion(tf.random.uniform(fake_output.shape, minval=0, maxval=0.1), fake_output)\n",
    "    return real_loss + fake_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "fx5Y2Fgn6dSc"
   },
   "outputs": [],
   "source": [
    "def generator_loss(fake_output):\n",
    "    # return criterion(tf.ones_like(fake_output), fake_output)\n",
    "    return criterion(tf.random.uniform(fake_output.shape, minval=0.9, maxval=1), fake_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "cfABJjd76faR"
   },
   "outputs": [],
   "source": [
    "gen_opt = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n",
    "disc_opt = tf.keras.optimizers.Adam(4e-4, beta_1=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "WlcPxkok7GEk"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from time import time\n",
    "\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint = tf.train.Checkpoint(gen_opt=gen_opt, disc_opt=disc_opt, g=g, d=d)\n",
    "manager = tf.train.CheckpointManager(checkpoint, directory=checkpoint_dir, max_to_keep=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "WIngSa-h7c8X"
   },
   "outputs": [],
   "source": [
    "noise_dim = 100\n",
    "\n",
    "seed = tf.random.normal([16, noise_dim])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "84DTPS8JQMSG"
   },
   "outputs": [],
   "source": [
    "!mkdir timelapse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "5iM56Mjf7rwq"
   },
   "outputs": [],
   "source": [
    "def gen_grid(gen, show=True, save=False, idx=0):\n",
    "    fig, ax = plt.subplots(nrows=4,ncols=4,figsize=[16,16],gridspec_kw = {'wspace':0, 'hspace':0})\n",
    "    fig.patch.set_facecolor('white')\n",
    "    for i,axi in enumerate(ax.flat):\n",
    "        axi.axis(\"off\")\n",
    "        axi.imshow((gen[i]+1)/2)\n",
    "    if save:\n",
    "        fig.savefig(\"./timelapse/\"+str(int(time()))+\".png\", bbox_inches='tight')\n",
    "    if show:\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "_eTa5R_c70tg"
   },
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(images):\n",
    "    noise = tf.random.normal([BATCH_SIZE, noise_dim])\n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "        generated = g(noise, training=True)\n",
    "\n",
    "        real_output = d(images, training=True)\n",
    "        fake_output = d(generated, training=True)\n",
    "\n",
    "        gen_loss = generator_loss(fake_output)\n",
    "        disc_loss = discriminator_loss(real_output, fake_output)\n",
    "    gen_grads = gen_tape.gradient(gen_loss, g.trainable_variables)\n",
    "    disc_grads = disc_tape.gradient(disc_loss, d.trainable_variables)\n",
    "\n",
    "    gen_opt.apply_gradients(zip(gen_grads, g.trainable_variables))\n",
    "    disc_opt.apply_gradients(zip(disc_grads, d.trainable_variables))\n",
    "    return gen_loss, disc_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "wDswdPFpVvBf",
    "outputId": "eb509da9-5ca4-4f01-b09b-69a6089c23e3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: cannot remove 'timelapse/*': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!rm timelapse/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "g4rIGCKtDO27"
   },
   "outputs": [],
   "source": [
    "def train(dataset):\n",
    "    steps = len(dataset)\n",
    "    start = time()\n",
    "    i=0\n",
    "    for batch in dataset:\n",
    "        gen_loss, disc_loss = train_step(batch)\n",
    "        if not i%100:\n",
    "            print(f\"\\r{i}/{steps}\\t Time: {time()-start:.2f}s\\t Gen Loss: {gen_loss:.5f}\\t Desc Loss: {disc_loss:.5f}\", end=\"    \")\n",
    "            if not i%1_000:\n",
    "                noise = tf.random.normal([4, noise_dim])\n",
    "                gen = g(noise).numpy()\n",
    "                cnv = np.hstack(gen)\n",
    "                fig = plt.figure(figsize=[16, 64])\n",
    "                plt.imshow((cnv+1)/2)\n",
    "                plt.axis('off')\n",
    "                plt.show()\n",
    "                gen = g(seed)\n",
    "                gen_grid(gen, show=False, save=True, idx=i)\n",
    "                print(\"Grid Saved.\")\n",
    "            if not i%7_000:\n",
    "                manager.save()\n",
    "                print(\"Checkpoint saved.\")\n",
    "        i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 881
    },
    "id": "Ceulvyo2F7-2",
    "outputId": "606ac3b9-3b7f-4f53-baab-b3df19dba469"
   },
   "outputs": [],
   "source": [
    "EPOCHS = 50\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f\"Epoch {epoch+1} of {EPOCHS}.\")\n",
    "    train(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cLZ1hdIFF9Ug"
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import imageio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WwsdG9flimsH"
   },
   "outputs": [],
   "source": [
    "anim_file = 'dcgan.gif'\n",
    "\n",
    "with imageio.get_writer(anim_file, mode='I') as writer:\n",
    "    filenames = glob.glob('timelapse/*.png')\n",
    "    filenames = sorted(filenames)\n",
    "    for filename in filenames:\n",
    "        image = imageio.imread(filename)\n",
    "        writer.append_data(image)\n",
    "    image = imageio.imread(filename)\n",
    "    writer.append_data(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PWLP8aDymXhz"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "celeb_face_GAN.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
